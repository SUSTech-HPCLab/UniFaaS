import logging
import os
from unifaas.dataflow.helper.execution_recorder import UNIFAAS_HOME
from unifaas.dataflow.helper.resource_status_poller import ResourceStatusPoller
from unifaas.executors.funcx.executor import FuncXExecutor
from funcx.sdk.file import RemoteFile, RemoteDirectory
import pandas as pd
import numpy as np
import pickle

# sklearnx is an extension for Intel CPU
from sklearnex import patch_sklearn
patch_sklearn(verbose=False)
logging.getLogger("sklearnex").setLevel(logging.CRITICAL)
from sklearn.ensemble import RandomForestRegressor
import concurrent.futures
import time
from unifaas.dataflow.helper.execution_recorder import write_workflow_prediction_result
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from concurrent.futures import Future

logger = logging.getLogger("unifaas")
exp_logger = logging.getLogger("experiment")



class ExecutionPredictor:
    """ExecutionPredictor is a class that predicts the execution time of a task
        - Read the log generated by ExecutionRecorder
        - Build a model to predict the execution time of a task
        - Update the model periodically
        - Predict the execution time of a task by input
        - Input: task, executor status, etc.
    """

    def __init__(self, executors, recorder, workflow_name):
        self.executors = executors
        self.recorder = recorder
        self.status_poller = ResourceStatusPoller(executors)
        self.record_dir = os.path.join(UNIFAAS_HOME, "execution_history.db")
        self.predict_model_dir = os.path.join(UNIFAAS_HOME, "predict_model")
        self.execution_model_dir = os.path.join(self.predict_model_dir, "execution_model")
        self.special_model_dir = os.path.join(self.predict_model_dir, "special_model")
        self.output_model_dir = os.path.join(self.predict_model_dir, "output_model")
        self.add_root_logger_handler()
        self.model_map = {}  # {"func_name" : model}
        self.output_model_map = {}
        self.special_model_map = {}
        self._init_model()
        self.workflow_name = workflow_name # this variable can infecct the file path of the prediction result
        self.workflow_path = os.path.join(UNIFAAS_HOME, "workflow", self.workflow_name)

        logger.info(f"[ExecutionPredictor] Start ExecutionPredictor with dir{self.record_dir}")
        
        #self._special_training()

    def add_root_logger_handler(self):
        '''
        **IMPORTANT**
        sklearnex will add a handler to root logger if there is no handler
        The handler added by sklearnex is very verbose
        So we need to add a high level handler to root logger to suppress the log
        '''
        root_logger = logging.getLogger()
        handler = logging.StreamHandler()
        handler.setLevel(logging.CRITICAL)
        root_logger.addHandler(handler)

    def calculate_func_input_size(self,task_record):
        import sys
        input_size = 0

        def cal_remote_file_size(d):
            try:
                if isinstance(d, Future) and d.done():
                    d = d.result()

                if isinstance(d, list) or isinstance(d, tuple):
                    res = 0
                    for sub_d in d:
                        res += cal_remote_file_size(sub_d)
                    return res
                elif isinstance(d, dict):
                    res = 0
                    for key in d.keys():
                        res += cal_remote_file_size(d[key])
                    return res
                elif isinstance(d, RemoteFile):
                    return d.get_file_size()
                elif isinstance(d, RemoteDirectory):
                    return d.get_directory_size()
                else:
                    return sys.getsizeof(d)
            except Exception as e:
                return 0

        for arg in task_record['args']:
            input_size += cal_remote_file_size(arg)
        for key in task_record['kwargs'].keys():
            input_size += cal_remote_file_size(task_record['kwargs'][key])
        try:
            for dep in task_record['depends']:
                if dep.done():
                    continue
                dep_record =  dep.task_def
                if "predict_output" in dep_record.keys():
                    input_size += dep_record['predict_output']
                else:
                    self.predict(dep_record)
                    input_size += dep_record['predict_output']
        except Exception as e:
            return 0

        return input_size




    def _special_training(self):
        data = self.recorder.select_record_for_cpu_combination()
        for cpu_comb in data:
            for func in data[cpu_comb]:
                if len(data[cpu_comb][func]) < 5:
                    continue
                else:
                    record_items = list(data[cpu_comb][func])
                    X_list  = [ [item[2]] for item in record_items ]
                    Y_list  = [ item[8] for item in record_items ]
                    X = np.array(X_list)
                    Y = np.array(Y_list)
                    # ==== Random Forest =====
                    # regressor =  RandomForestRegressor(n_estimators=200, random_state=0)
                    # regressor.fit(X, Y)

                    # ==== Linear Regression =====
                    poly_features = PolynomialFeatures(degree=2)
                    X_poly = poly_features.fit_transform(X)
                    regressor = LinearRegression()
                    regressor.fit(X_poly, Y)

                    my_file_name = f"{func}.pkl"
                    model_dir = os.path.join(self.special_model_dir, cpu_comb)
                    if not os.path.exists(model_dir):
                        os.mkdir(model_dir)

                    with open(os.path.join(model_dir, my_file_name), "wb") as f:
                        logger.info(f"[ExecutionPredictor] Training finished. Save model to {os.path.join(model_dir, my_file_name)}")
                        pickle.dump(regressor, f)
        



    def _init_model(self):
        # If there is no model directory, create a new one
        if not os.path.exists(self.predict_model_dir):
            os.mkdir(path=self.predict_model_dir)

        if not os.path.exists(self.execution_model_dir):
            os.mkdir(path=self.execution_model_dir)
        
        if not os.path.exists(self.output_model_dir):
            os.mkdir(path=self.output_model_dir)

        if not os.path.exists(self.special_model_dir):
            os.mkdir(path=self.special_model_dir)
        
        # If there is no model file, but there is a record file, train a model
        all_distinct_func = self.recorder.get_all_distinct_func()
        func_to_train = self._check_to_train()
        self.train(func_to_train)
        # Load existing model
        self._load_existing_model()
        
    
    def predict(self, task_record, latency=0):
        fx_executors, executor_info = self._get_executors_info()
        input_size = self.calculate_func_input_size(task_record)
        func_name = task_record["func_name"]
       
        predict_execution = {}
        predict_output = 0
        if func_name in self.model_map.keys():
            for exe_key in fx_executors:
                cpu_str = None
                if 'worker_performance' in executor_info[exe_key]:
                    if len(executor_info[exe_key]['worker_performance']) > 0 :
                        cpu_str = executor_info[exe_key]['worker_performance'][0]
                

                if cpu_str:
                    if cpu_str in self.special_model_map and func_name in self.special_model_map[cpu_str]:
                        model = self.special_model_map[cpu_str][func_name]
                        X = np.array([input_size])
                        poly_features = PolynomialFeatures(degree=2)
                        X_poly = poly_features.fit_transform(X.reshape(1, -1))
                        Y = model.predict(X_poly)
                        predict_execution[exe_key] = max(0,Y[0] + latency)
                        continue
                    
                X = np.array(
                    [input_size, executor_info[exe_key]["cpu_freqs_max"]]
                    )
                model = self.model_map[func_name]

                poly_features = PolynomialFeatures(degree=2)
                X_poly = poly_features.fit_transform(X.reshape(1, -1))
                Y = model.predict(X_poly)
                        
                predict_execution[exe_key] = max(0,Y[0] + latency)
        if func_name in self.output_model_map.keys():
            X = np.array([input_size])
            model = self.output_model_map[func_name]
            Y = model.predict(X.reshape(1, -1))
            predict_output = Y[0]
 
        task_record["predict_execution"] = predict_execution
        task_record["predict_output"] = predict_output
        task_record['input_size'] = input_size
        info = (func_name, input_size, predict_output, predict_execution)
        write_workflow_prediction_result(self.workflow_path,info)
        return (func_name, input_size, predict_execution, predict_output)

    def _get_executors_info(self):
        fx_executors = [] # list of FuncXExecutor's label
        executor_info =  {} # dict of executor's info like cpu/ meminfo
        for exe_key in self.executors.keys():
                funcx_executor = self.executors[exe_key]
                if isinstance(funcx_executor, FuncXExecutor):
                    fx_executors.append(exe_key)
                    info = self.status_poller.get_resource_status_by_label(exe_key)
                    tmp_dict = {}
                    tmp_dict["cpu_freqs_max"] = info["cpu_freq"]
                    tmp_dict["avail_mem"] = info["avail_mem"]
                    tmp_dict['cpu_percent'] = info['user_cpu'] + info['system_cpu']
                    tmp_dict['cpu_cores'] = info['cores']
                    if 'worker_performance' in info:
                        tmp_dict['worker_performance'] = info['worker_performance']

                    executor_info[exe_key] = tmp_dict
        return (fx_executors, executor_info)


    def look_up_history_model(self, func_set):
        in_history = []
        out_history = []
        for func in func_set:
            if func not in self.model_map.keys():
                out_history.append(func)
            else:
                in_history.append(func)
        return (in_history, out_history)

    def update(self):
        pass

    def _check_to_train(self):
        """
        If there is no model object at model dir, however there is a record file.
        Then train a model for the function, based on the record file
        """
        functions_list = self.recorder.get_all_distinct_func()
        import os
        func_to_train = []
        model_file_list = os.listdir(self.execution_model_dir)

        for func_name in functions_list:
            if f"{func_name}.pkl" not in model_file_list:
                func_to_train.append(func_name)
        return func_to_train

    def _load_existing_model(self):
        model_file_list = os.listdir(self.execution_model_dir)
        for tmp_file in model_file_list:
            func_name = tmp_file[:-len(".pkl")]
            with open(os.path.join(self.execution_model_dir, tmp_file), "rb") as f:
                self.model_map[func_name] = pickle.load(f)
            with open(os.path.join(self.output_model_dir, tmp_file), "rb") as f:
                self.output_model_map[func_name] = pickle.load(f)

        special_model_dir_list = os.listdir(self.special_model_dir)
        for special_dir in special_model_dir_list:
            cpu_str = special_dir
            self.special_model_map[cpu_str] = {}
            special_dir = os.path.join(self.special_model_dir, special_dir)
            model_list = os.listdir(special_dir)
            for model in model_list:
                func_name = model[:-len(".pkl")]
                with open(os.path.join(special_dir, model), "rb") as f:
                    self.special_model_map[cpu_str][func_name] = pickle.load(f)
          
                


    def train(self, func_to_train, init_flag=True):
        if isinstance(func_to_train, list) and len(func_to_train) == 0:
            return
        for func in func_to_train:
            self.model_map[func] = self._train_for_func(func)
            self.output_model_map[func] = self._train_output_model(func)
        

    def _train_for_func(self, func_name):
        record_items = self.recorder.get_record_by_func(func_name)
        X_list  = [ [item[2], item[6] ]for item in record_items ]
        Y_list  = [ item[8] for item in record_items ]
        X = np.array(X_list)
        Y = np.array(Y_list)
        # regressor = RandomForestRegressor(n_estimators=200, random_state=0)
        # regressor.fit(X, Y)

        # ==== Linear Regression =====
        poly_features = PolynomialFeatures(degree=2)
        X_poly = poly_features.fit_transform(X)
        regressor = LinearRegression()
        regressor.fit(X_poly, Y)


        with open(os.path.join(self.execution_model_dir, f"{func_name}.pkl"), "wb") as f:
            logger.info(f"[ExecutionPredictor] Training finished. Save model to {self.predict_model_dir}")
            pickle.dump(regressor, f)
        return regressor

    def _train_output_model(self, func_name):
        record_items = self.recorder.get_record_by_func(func_name)
        X_list  = [ [item[2]] for item in record_items ]
        Y_list  = [ item[9] for item in record_items ]
        X = np.array(X_list)
        Y = np.array(Y_list)
        regressor =  RandomForestRegressor(n_estimators=200, random_state=0)
        regressor.fit(X, Y)
        with open(os.path.join(self.output_model_dir, f"{func_name}.pkl"), "wb") as f:
            logger.info(f"[ExecutionPredictor] Training output model. Save model to {self.predict_model_dir}")
            pickle.dump(regressor, f)
        return regressor